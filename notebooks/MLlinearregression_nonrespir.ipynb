{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97341366-b73a-443b-bf29-0ae9a3ec1541",
   "metadata": {},
   "source": [
    "# Non Res PIR linear regression model\n",
    "\n",
    "### Linear regression model to convert non res PIR people figures to Filled post estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fb7f8d5-d6dc-40fc-b1ba-9a6bc3d595d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import io\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b99dbf-f028-48d4-90c6-d581af4b6945",
   "metadata": {},
   "source": [
    "Specify the bucket and the source path of the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea9ac20-7a18-40c1-8c0e-11b0f43fd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'sfc-mt-sagemaker-demo'                                                                                    #used for demo purposes \n",
    "DATA = 's3://sfc-main-datasets/domain=ind_cqc_filled_posts/dataset=ind_cqc_estimated_missing_ascwds_filled_posts/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ed861-a8a3-4a37-851f-48e1f7605c29",
   "metadata": {},
   "source": [
    "### Read AWS Glue Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44443caa-caba-487d-b5e8-f4d8fe59b222",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlueSchemaReader\n\u001b[1;32m      2\u001b[0m gsr \u001b[38;5;241m=\u001b[39m GlueSchemaReader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain-data-engineering-database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utilities'"
     ]
    }
   ],
   "source": [
    "from utilities.schema_reader import GlueSchemaReader\n",
    "gsr = GlueSchemaReader(\"main-data-engineering-database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a81658-5d75-40a9-bc5d-3af91d481cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plschema = gsr.get_polars_schema(\"dataset_ind_cqc_estimated_missing_ascwds_filled_posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7848c-05c6-4da7-8333-36cf8a7aa8b6",
   "metadata": {},
   "source": [
    "### Read data from S3 (use schema when sorted) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e2cda-bbb1-47fe-9895-a9a324207904",
   "metadata": {},
   "source": [
    "When reading the data with ```scan_parquet``` it reads it as a [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html) which you can filter.\n",
    "\n",
    "Ahead of using the data in a model you will need to ```collect()``` the data as shown further below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb589f2-7423-4449-a263-366c96286de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pl.scan_parquet(DATA,schema=plschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04489a44-1024-42af-a853-c54eb8b24b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e18bf5-a00f-4746-a130-b77e64fc0bbc",
   "metadata": {},
   "source": [
    "Filtering dataset to required data using Polars syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d5e211-1dc4-4ffb-bab9-fc3931a584b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_selected = df1.select(\"locationId\", \"cqc_location_import_date\", \"careHome\", \"ascwds_filled_posts_deduplicated_clean\" ,\"pir_people_directly_employed_deduplicated\")\n",
    "df1_filtered = df1_selected.filter(pl.col(\"careHome\") == \"N\")\n",
    "df1_filtered2 = df1_filtered.filter(\n",
    "    (pl.col(\"ascwds_filled_posts_deduplicated_clean\").is_not_null()) \n",
    "    & (pl.col(\"ascwds_filled_posts_deduplicated_clean\") > 0) \n",
    "    & (pl.col(\"pir_people_directly_employed_deduplicated\").is_not_null())\n",
    "    & (pl.col(\"pir_people_directly_employed_deduplicated\") > 0)\n",
    ")\n",
    "df1_filtered3 = df1_filtered2.with_columns(\n",
    "    (pl.col(\"ascwds_filled_posts_deduplicated_clean\") - pl.col(\"pir_people_directly_employed_deduplicated\")).abs().alias(\"abs_resid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fee64a-8110-42d2-bc2f-15f7e952d4cd",
   "metadata": {},
   "source": [
    "Test Dataset matches example model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137c54b-a05a-489e-9f86-e8570b6555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a date for 28,000 rows \n",
    "dftest = df1.filter(pl.col(\"import_date\") == 20250301)\n",
    "dftest.collect().shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2559ad2d-9c29-4c3d-b69a-2e1d4f0e0783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>locationId</th><th>cqc_location_import_date</th><th>careHome</th><th>ascwds_filled_posts_deduplicated_clean</th><th>pir_people_directly_employed_deduplicated</th><th>abs_resid</th></tr><tr><td>str</td><td>date</td><td>str</td><td>f64</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1-133394328&quot;</td><td>2023-04-01</td><td>&quot;N&quot;</td><td>548.0</td><td>82</td><td>466.0</td></tr><tr><td>&quot;1-8607670975&quot;</td><td>2023-10-01</td><td>&quot;N&quot;</td><td>567.0</td><td>135</td><td>432.0</td></tr><tr><td>&quot;1-4776578785&quot;</td><td>2023-08-01</td><td>&quot;N&quot;</td><td>998.0</td><td>612</td><td>386.0</td></tr><tr><td>&quot;1-3679714454&quot;</td><td>2025-01-01</td><td>&quot;N&quot;</td><td>973.0</td><td>615</td><td>358.0</td></tr><tr><td>&quot;1-12291638633&quot;</td><td>2023-01-01</td><td>&quot;N&quot;</td><td>374.0</td><td>23</td><td>351.0</td></tr><tr><td>&quot;1-6957740561&quot;</td><td>2022-07-01</td><td>&quot;N&quot;</td><td>460.0</td><td>125</td><td>335.0</td></tr><tr><td>&quot;1-1678280100&quot;</td><td>2024-12-01</td><td>&quot;N&quot;</td><td>730.0</td><td>422</td><td>308.0</td></tr><tr><td>&quot;1-2470563606&quot;</td><td>2025-01-01</td><td>&quot;N&quot;</td><td>46.0</td><td>314</td><td>268.0</td></tr><tr><td>&quot;1-2013534526&quot;</td><td>2024-10-08</td><td>&quot;N&quot;</td><td>330.0</td><td>99</td><td>231.0</td></tr><tr><td>&quot;1-6957740561&quot;</td><td>2024-03-01</td><td>&quot;N&quot;</td><td>340.5</td><td>113</td><td>227.5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌───────────────┬───────────────────┬──────────┬───────────────────┬───────────────────┬───────────┐\n",
       "│ locationId    ┆ cqc_location_impo ┆ careHome ┆ ascwds_filled_pos ┆ pir_people_direct ┆ abs_resid │\n",
       "│ ---           ┆ rt_date           ┆ ---      ┆ ts_deduplicat…    ┆ ly_employed_d…    ┆ ---       │\n",
       "│ str           ┆ ---               ┆ str      ┆ ---               ┆ ---               ┆ f64       │\n",
       "│               ┆ date              ┆          ┆ f64               ┆ i32               ┆           │\n",
       "╞═══════════════╪═══════════════════╪══════════╪═══════════════════╪═══════════════════╪═══════════╡\n",
       "│ 1-133394328   ┆ 2023-04-01        ┆ N        ┆ 548.0             ┆ 82                ┆ 466.0     │\n",
       "│ 1-8607670975  ┆ 2023-10-01        ┆ N        ┆ 567.0             ┆ 135               ┆ 432.0     │\n",
       "│ 1-4776578785  ┆ 2023-08-01        ┆ N        ┆ 998.0             ┆ 612               ┆ 386.0     │\n",
       "│ 1-3679714454  ┆ 2025-01-01        ┆ N        ┆ 973.0             ┆ 615               ┆ 358.0     │\n",
       "│ 1-12291638633 ┆ 2023-01-01        ┆ N        ┆ 374.0             ┆ 23                ┆ 351.0     │\n",
       "│ 1-6957740561  ┆ 2022-07-01        ┆ N        ┆ 460.0             ┆ 125               ┆ 335.0     │\n",
       "│ 1-1678280100  ┆ 2024-12-01        ┆ N        ┆ 730.0             ┆ 422               ┆ 308.0     │\n",
       "│ 1-2470563606  ┆ 2025-01-01        ┆ N        ┆ 46.0              ┆ 314               ┆ 268.0     │\n",
       "│ 1-2013534526  ┆ 2024-10-08        ┆ N        ┆ 330.0             ┆ 99                ┆ 231.0     │\n",
       "│ 1-6957740561  ┆ 2024-03-01        ┆ N        ┆ 340.5             ┆ 113               ┆ 227.5     │\n",
       "└───────────────┴───────────────────┴──────────┴───────────────────┴───────────────────┴───────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_filtered3.sort(pl.col(\"abs_resid\"), descending=True).collect().head(10)\n",
    "\n",
    "# shows same data and EMR noteboook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e3dde-da67-4ed0-95f3-faaddb49aa3b",
   "metadata": {},
   "source": [
    "Create filtered and excluded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44181804-9c42-45a2-9b79-3dd00b3dbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df1_filtered3.filter(pl.col(\"abs_resid\") <= 500).drop(\"abs_resid\")\n",
    "excluded_df = df1_filtered3.filter(pl.col(\"abs_resid\") > 500).drop(\"abs_resid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10663567-10c3-457b-81ed-f27e2f2489ef",
   "metadata": {},
   "source": [
    "Previously within EMR, Pyspark required vectorising of the data for modelling.\n",
    "\n",
    "As the dataset is a small enough this is not required in scikitlearn ahead of linear regression but if needed for more complicated models in the future Please seee below the syntax required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823f67b-b02b-448b-aaa7-e4d7b96d5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorised_features_df = DictVectorizer()     Does what the VectorAssembler() did in pyspark - required for pyspark model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad447c-59fc-4df2-9788-b8c9cc90a040",
   "metadata": {},
   "source": [
    "## Separate into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b0f9e-a9e0-48d7-b743-c6d627c68cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FRAC = 0.8\n",
    "df_eager = filtered_df.lazy().collect()                                                          #this converts the lazyframe to a dataframe (this is needed for the modeling)\n",
    "df_train1 = df_eager.sample(fraction=TRAIN_FRAC, with_replacement=False, shuffle=True, seed=55)\n",
    "df_test1 = df_eager.join(df_train1, on=df_eager.columns, how='anti')\n",
    "\n",
    "# Go through and explain each part of the code and what it does "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada9860-5e16-4cc2-a257-38418b913980",
   "metadata": {},
   "source": [
    "## Training\n",
    "1. Put the data into numpy arrays for use by scikit-learn\n",
    "2. Create and fit the model\n",
    "3. Get the predicted values from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c305dbf-01b5-4ad1-812b-085b6345658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_train1['pir_people_directly_employed_deduplicated'].to_numpy().reshape(-1,1)   #input feature (Reshapes the 1D array into a 2D column vector, which is required by sklearn.)\n",
    "y1 = df_train1['ascwds_filled_posts_deduplicated_clean'].to_numpy()                    #target variable\n",
    "model1 = LinearRegression()\n",
    "model1.fit(x1, y1)                                                                     #fits the linear regression model\n",
    "y_pred1 = model1.predict(x1)                                                           #predicts filled posts based on people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d1fb8-0f81-4bf4-8272-31862fc34cbe",
   "metadata": {},
   "source": [
    "Scikitlearn offers a wide range of [Linear models](https://scikit-learn.org/stable/modules/linear_model.html) that can be imported for more complicated models. \n",
    "\n",
    "An example of others you used in this simple model is [Polynomial Regression](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28028327-9f75-42bb-8dc7-43b68c3e1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x1, y1, alpha=0.5, s=10, label='pir_people_directly_employed_deduplicated')\n",
    "ax.plot(x1, y_pred1, color='red', linewidth=2, label='Linear Regression')\n",
    "# ax.set_title('')\n",
    "ax.set_xlabel('pir_people_directly_employed_deduplicated')\n",
    "ax.set_ylabel('ascwds_filled_posts_deduplicated_clean')\n",
    "ax.ticklabel_format(style='plain', axis='y')\n",
    "ax.grid(True, linestyle=':', alpha=0.7)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ask gary to take through each of the regressions he used - I can add this but not a priority \n",
    "# find list of each of these within "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c4b17-da07-4889-9178-11d5afc2c943",
   "metadata": {},
   "source": [
    "### Verify that the R^{2} is good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd5442-72dd-4071-8c3c-a23b9a77eec4",
   "metadata": {},
   "source": [
    "Specify the bucket and the source path of the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50417bfa-708d-46d6-9738-3abd71106e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_1 = r2_score(y1, y_pred1)\n",
    "\n",
    "print(f'The model has an R2 score {r2_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb91e6-4970-431b-a61c-714a6c1a660d",
   "metadata": {},
   "source": [
    "## Check model against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f61a3-ba7e-4b0f-9328-091e8e2f8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = df_test1['pir_people_directly_employed_deduplicated'].to_numpy().reshape(-1,1)\n",
    "y_test1 = df_test1['ascwds_filled_posts_deduplicated_clean'].to_numpy()\n",
    "y_test_pred1 = model1.predict(x_test1)\n",
    "r2_test1 = r2_score(y_test1, y_test_pred1)\n",
    "print(f'The model scores {r2_test1} on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b514f-b9ee-4ae8-a764-0b95461c6fcd",
   "metadata": {},
   "source": [
    "Extract the model parameters if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e2d19-7437-48ba-905c-3b5836a87e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = model1.coef_[0]\n",
    "intercept = model1.intercept_\n",
    "print(f'The model slope is {slope}, and the intercept is {intercept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28843112-7cfe-4243-b6d2-83d56c21ac2c",
   "metadata": {},
   "source": [
    "## Save the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ed304-ce11-412f-94a3-5a7ad2715534",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.delete_object(Bucket='sfc-mt-sagemaker-demo', Key='params/v1/params.pkl')\n",
    "    print(\"Deleted params/v1/params.pkl from sfc-mt-sagemaker-demo\")\n",
    "    s3.delete_object(Bucket='sfc-mt-sagemaker-demo', Key='params/v2/params.pkl')\n",
    "    print(\"Deleted params/v2/params.pkl from sfc-mt-sagemaker-demo\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Deletion failed, possibly no files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cc7f2-b01c-4c1c-a2a7-2e3365622e76",
   "metadata": {},
   "source": [
    "We will serialise the model using the built-in [pickle](https://docs.python.org/3/library/pickle.html) module. For a simple linear model, it is possible just to save the model parameters in plain text, but it is better practice to save the full model instance for re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33bbc7-21a3-4058-8ad5-91d8af37e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'sfc-mt-sagemaker-demo'\n",
    "\n",
    "buffer = io.BytesIO()                                     # creates in memory binary stream \n",
    "pickle.dump(model1, buffer)\n",
    "buffer.seek(0)                                            # moves the 'cursor' to the start of the buffer \n",
    "s3.upload_fileobj(buffer, BUCKET, 'params/v1/params.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b13a33-90ae-4de8-bbfe-ec7bf3cf1ded",
   "metadata": {},
   "source": [
    "## New data\n",
    "\n",
    "Now we imagine that some time has passed and new data is available. First the analyst reads in the model that has been saved to S3. Then s/he reads in the new data and compares it to the model predictions. If these turn out to be a poor fit, the training/testing/serialisation cycle is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a4e96-f084-4988-85c4-c2efeea7c894",
   "metadata": {},
   "source": [
    "## Retrieve the serialised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c91c21-1542-4349-ac96-3f5711b6c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "download = io.BytesIO()\n",
    "s3.download_fileobj(BUCKET, 'params/v1/params.pkl', download)\n",
    "download.seek(0)\n",
    "loaded_model = pickle.load(download)\n",
    "loaded_model.coef_[0], loaded_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609c52c-5aef-4d8f-bc4b-bb42d419e414",
   "metadata": {},
   "source": [
    "## Retrieve the latest data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337040e-ffe2-4998-8517-bbd7a917e9b4",
   "metadata": {},
   "source": [
    "- Set out the new data location within the S3 bucket.\n",
    "- Use ```scan_parquet()``` to pull the data in as a Lazyframe, ```collect()``` data to convert to eager dataframe in order to compare the model predicitons to the new data.\n",
    "- Plot the data as required/ as above.\n",
    "- Repeat Training, Testing and Serialisation.\n",
    "- Save to S3 as new version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0299883-d93b-4fb2-b9a9-09f616de6ef2",
   "metadata": {},
   "source": [
    "# Test Result of Polars vs Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c7e52-d5be-4dc3-8bd7-8c46c6f8efce",
   "metadata": {},
   "source": [
    "Test that the model using Polars produces the same model (intercept and gradient) as Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31627c7-6882-4698-8b6a-3178a6a12a6b",
   "metadata": {},
   "source": [
    "### As within Non Res PIR linear regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e1337-c994-4a28-bd8a-1dbd75cf342c",
   "metadata": {},
   "source": [
    "Using Polars:\n",
    "- Test using whole dataset in order to compare.\n",
    "- No randomness to the data used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28fcab1-f647-4a47-a256-94e828987f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FRAC_TEST = 1\n",
    "\n",
    "df_eager1 = filtered_df.lazy().collect()\n",
    "polars_test = df_eager1.sample(fraction=TRAIN_FRAC_TEST) # no need to input randomness in the dataset\n",
    "df_test = df_eager1.join(polars_test, on=df_eager1.columns, how='anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ee691f-2d13-40bd-9c0e-9bfa87db47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = polars_test['pir_people_directly_employed_deduplicated'].to_numpy().reshape(-1,1)   #input feature (Reshapes the 1D array into a 2D column vector, which is required by sklearn.)\n",
    "y1 = polars_test['ascwds_filled_posts_deduplicated_clean'].to_numpy()                    #target variable\n",
    "modeltest = LinearRegression()\n",
    "modeltest.fit(x1, y1)                                                                     #fits the linear regression model\n",
    "y_predtest = modeltest.predict(x1)                                                        #predicts filled posts based on people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06efa37f-d9bb-4755-a12e-95497f101763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model slope is 1.057218026189747, and the intercept is 5.174214629366368\n"
     ]
    }
   ],
   "source": [
    "slope = modeltest.coef_[0]\n",
    "intercept = modeltest.intercept_\n",
    "print(f'The model slope is {slope}, and the intercept is {intercept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a480c1f-1b14-45d7-b8d8-ff311dac9076",
   "metadata": {},
   "source": [
    "Using PySpark:\n",
    "- This has been carried out within EMR to test and save this result.\n",
    "- This is because EMR supports Pyspark as spark needs clusters.\n",
    "- Again, test using the whole dataset in order to be able to compare.\n",
    "- No Randomness to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100cf5a-105a-421f-9049-9c56d39fae15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
